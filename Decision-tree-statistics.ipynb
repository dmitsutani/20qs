{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2d1b06-e8b0-434e-9de5-25b504712403",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!{sys.executable} -m pip install openai\n",
    "!{sys.executable} -m pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3715cd-c529-4091-b517-a0ba4d8c44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from openai import OpenAI\n",
    "\n",
    "if not os.environ[\"OPENAI_API_KEY\"]: \n",
    "    os.environ[\"OPENAI_API_KEY\"]= '<REDACTED>'\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8b0c4",
   "metadata": {},
   "source": [
    "Baseline decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fb2991-c731-44f6-8db0-05414368e94c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: Is it a place?\n",
      " ├─ Yes:\n",
      "    → Examples: Lesotho, Acapulco, Oceania, Parking lot\n",
      "    Turn 2: Is it a country?\n",
      "     ├─ Yes:\n",
      "        → Examples: Brazil, Japan, Kenya, Norway\n",
      "        Turn 3: Is this country either in Asia or in Africa?\n",
      "         ├─ Yes:\n",
      "            → Examples: Thailand, North Korea, Egypt, Nigeria\n",
      "            Turn 4: Is this country in Asia?\n",
      "             ├─ Yes:\n",
      "                → Summary: The keyword is a country in Asia.\n",
      "                → Examples: Kazakhstan, South Korea\n",
      "             └─ No:\n",
      "                → Summary: The keyword is a country in Africa.\n",
      "                → Examples: Morocco, Ghana\n",
      "         └─ No:\n",
      "            → Examples: Lithuania, Papua New Guinea, Uruguay, Slovakia\n",
      "            Turn 4: Is this country in Europe?\n",
      "             ├─ Yes:\n",
      "                → Summary: The keyword is a country in Europe.\n",
      "                → Examples: Romania, Germany\n",
      "             └─ No:\n",
      "                → Examples: Australia, Nicaragua, Chile, United States\n",
      "                Turn 5: Is this country in the Americas?\n",
      "                 ├─ Yes:\n",
      "                    → Summary: The keyword is a country in the Americas.\n",
      "                    → Examples: Argentina, Canada\n",
      "                 └─ No:\n",
      "                    → Summary: The keyword is a country in Oceania.\n",
      "                    → Examples: New Zealand, Samoa\n",
      "     └─ No:\n",
      "        → Examples: Venice, Lagos, Times Square, Colosseum\n",
      "        Turn 3: Is it a city?\n",
      "         ├─ Yes:\n",
      "            → Examples: Paris, Astana, Alexandria, Sao Paulo\n",
      "            Turn 4: Is it a capital city?\n",
      "             ├─ Yes:\n",
      "                → Summary: The keyword is a capital city.\n",
      "                → Examples: Moscow, Santiago\n",
      "             └─ No:\n",
      "                → Summary: The keyword is a city which is not a capital.\n",
      "                → Examples: Chicago, Tianjin\n",
      "         └─ No:\n",
      "            → Examples: Oceania, Nevada, Times Square, Colosseum\n",
      "            Turn 4: Is it either a continent or a state of a country?\n",
      "             ├─ Yes:\n",
      "                → Examples: North America, Nevada, Roraima, Uttar Pradesh\n",
      "                Turn 5: Is it a continent?\n",
      "                 ├─ Yes:\n",
      "                    → Summary: The keyword is a continent.\n",
      "                    → Examples: Africa, Asia\n",
      "                 └─ No:\n",
      "                    → Summary: The keyword is a state of a country.\n",
      "                    → Examples: Kentucky, Santa Catarina Brazil\n",
      "             └─ No:\n",
      "                → Examples: Copacabana beach, university campus, Antarctica, tundra\n",
      "                Turn 5: Is it a location made by humans?\n",
      "                 ├─ Yes:\n",
      "                    → Summary: The keyword is a location made by humans.\n",
      "                    → Examples: Champs-Elysees, Amusement park\n",
      "                 └─ No:\n",
      "                    → Summary: The keyword is a natural location.\n",
      "                    → Examples: Amazon rainforest, Desert\n",
      " └─ No:\n",
      "    → Examples: Pencil sharpener, cloud, chimney, electric vehicle\n",
      "    Turn 2: Is it man-made?\n",
      "     ├─ Yes:\n",
      "        → Examples: plastic bottle, martini, earbuds, helicopter\n",
      "        Turn 3: Is it found indoors?\n",
      "         ├─ Yes:\n",
      "            → Examples: pencil sharpener, refrigerator, coffee machine, escalator\n",
      "            Turn 4: Is it something found in a home?\n",
      "             ├─ Yes:\n",
      "                → Summary: The keyword is a man-made thing found in people's homes.\n",
      "                → Examples: Bed frame, Stove\n",
      "             └─ No:\n",
      "                → Summary: The keyword is a man-made thing found indoors but not in people's homes.\n",
      "                → Examples: Hospital bed, Erlenmeyer flask\n",
      "         └─ No:\n",
      "            → Examples: pickup truck, train, fountain, obelisk\n",
      "            Turn 4: Is it something found in a city?\n",
      "             ├─ Yes:\n",
      "                → Summary: The keyword is a man-made thing found outdoors in cities.\n",
      "                → Examples: Apartment Building, Fire Hydrant\n",
      "             └─ No:\n",
      "                → Summary: The keyword is a man-made thing found outdoors outside of cities.\n",
      "                → Examples: Satellite, Barn\n",
      "     └─ No:\n",
      "        → Examples: thunder, Acacia, giraffe, river\n",
      "        Turn 3: Is it a living thing?\n",
      "         ├─ Yes:\n",
      "            → Examples: Palm tree, beetle, whale, apple\n",
      "            Turn 4: Is it an animal?\n",
      "             ├─ Yes:\n",
      "                → Examples: sea lion, cape toad, kangaroo, peacock\n",
      "                Turn 5: Is it a mammal or a bird?\n",
      "                 ├─ Yes:\n",
      "                    → Summary: The keyword is either a mammal or a bird.\n",
      "                    → Examples: Elephant, Seagull\n",
      "                 └─ No:\n",
      "                    → Summary: The keyword is either a fish, insect, reptilian or amphibian.\n",
      "                    → Examples: Shark, Wasp, Frog, Alligator\n",
      "             └─ No:\n",
      "                → Examples: mold, deciduous tree, yeast, agave\n",
      "                Turn 5: Is it a plant?\n",
      "                 ├─ Yes:\n",
      "                    → Examples: orchid, poison ivy, maple tree, sunflower\n",
      "                    Turn 6: Is it a tree?\n",
      "                     ├─ Yes:\n",
      "                        → Summary: The keyword is a tree.\n",
      "                        → Examples: Oak tree, Cactus\n",
      "                     └─ No:\n",
      "                        → Summary: The keyword is a plant which is not a tree.\n",
      "                        → Examples: Rose, Fern\n",
      "                 └─ No:\n",
      "                    → Examples: black mold, truffle, cremini mushroom, bacillus\n",
      "                    Turn 6: Is it a fungus?\n",
      "                     ├─ Yes:\n",
      "                        → Summary: The keyword is a fungus.\n",
      "                        → Examples: Wood ear mushroom, Yeast\n",
      "                     └─ No:\n",
      "                        → Summary: The keyword is a bacteria.\n",
      "                        → Examples: E Coli, Streptococcus\n",
      "         └─ No:\n",
      "            → Examples: uranus, seine river, carbon, electron\n",
      "            Turn 4: Is it a geological feature?\n",
      "             ├─ Yes:\n",
      "                → Examples: Mount Fuji, volcano, canyon, Rocky mountains\n",
      "                Turn 5: Is it a body of water?\n",
      "                 ├─ Yes:\n",
      "                    → Summary: The keyword is a body of water.\n",
      "                    → Examples: River, Mediterranean Sea\n",
      "                 └─ No:\n",
      "                    → Summary: The keyword is a geological feature which is not a body of water.\n",
      "                    → Examples: Cave, Volcano\n",
      "             └─ No:\n",
      "                → Examples: mercury, sun, iron, lead\n",
      "                Turn 5: Is it a chemical element, molecule or compound?\n",
      "                 ├─ Yes:\n",
      "                    → Summary: The keyword is a chemical element, molecule or compound.\n",
      "                    → Examples: Salt, Oxygen\n",
      "                 └─ No:\n",
      "                    → Summary: The keyword is a non-living thing not made by humans.\n",
      "                    → Examples: Pebble, Milky Way\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    def __init__(self, question=None, tag = None, yes_branch=None, no_branch=None, examples=None, summary=None):\n",
    "        self.question = question\n",
    "        self.yes_branch = yes_branch\n",
    "        self.no_branch = no_branch\n",
    "        self.examples = examples\n",
    "        self.summary = summary\n",
    "        self.tag = tag\n",
    "\n",
    "def print_tree(node, indent=0):\n",
    "    \"\"\" Recursively print the decision tree. \"\"\"\n",
    "    if node is None:\n",
    "        return\n",
    "    if node.summary:\n",
    "        # Print leaf node summary\n",
    "        print(' ' * indent + f'→ Summary: The keyword is {node.summary}.')\n",
    "    if node.examples:\n",
    "        # Print leaf node examples\n",
    "        print(' ' * indent + f'→ Examples: {\", \".join(node.examples)}')\n",
    "    if node.question:\n",
    "        # Print the question node\n",
    "        turn = indent // 4 + 1\n",
    "        print(' ' * indent + f'Turn {turn}: {node.question}')\n",
    "        if node.yes_branch or node.no_branch:\n",
    "            print(' ' * indent + ' ├─ Yes:')\n",
    "            print_tree(node.yes_branch, indent + 4)  # Print yes branch\n",
    "            print(' ' * indent + ' └─ No:')\n",
    "            print_tree(node.no_branch, indent + 4)   # Print no branch\n",
    "\n",
    "# Define the decision tree structure\n",
    "tree = Node(\n",
    "    \"Is it a place?\",\n",
    "    tag = \"place\",\n",
    "    yes_branch=Node(\n",
    "        examples=['Lesotho', 'Acapulco', 'Oceania', 'Parking lot'],\n",
    "        question=\"Is it a country?\",\n",
    "        tag = \"country\",\n",
    "        yes_branch=Node(\n",
    "            examples=['Brazil', 'Japan', 'Kenya', 'Norway'],\n",
    "            question=\"Is this country either in Asia or in Africa?\",\n",
    "            yes_branch=Node(\n",
    "                examples=['Thailand', 'North Korea', 'Egypt', 'Nigeria'],\n",
    "                question=\"Is this country in Asia?\",\n",
    "                yes_branch=Node(summary='a country in Asia', examples=['Kazakhstan', 'South Korea']),\n",
    "                no_branch=Node(summary='a country in Africa', examples=['Morocco', 'Ghana'])\n",
    "            ),\n",
    "            no_branch=Node(\n",
    "                examples=['Lithuania', 'Papua New Guinea', 'Uruguay', 'Slovakia'],\n",
    "                question=\"Is this country in Europe?\",\n",
    "                yes_branch=Node(summary='a country in Europe', examples=['Romania', 'Germany']),\n",
    "                no_branch=Node(\n",
    "                    examples=['Australia', 'Nicaragua', 'Chile', 'United States'],\n",
    "                    question=\"Is this country in the Americas?\",\n",
    "                    yes_branch=Node(summary='a country in the Americas', examples=['Argentina', 'Canada']),\n",
    "                    no_branch=Node(summary='a country in Oceania', examples=['New Zealand', 'Samoa'])\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        no_branch=Node(\n",
    "            examples=['Venice', 'Lagos', 'Times Square', 'Colosseum'],\n",
    "            question=\"Is it a city?\",\n",
    "            tag = \"city\",\n",
    "            yes_branch=Node(\n",
    "                examples=['Paris', 'Astana', 'Alexandria', 'Sao Paulo'],\n",
    "                question=\"Is it a capital city?\",\n",
    "                yes_branch=Node(summary='a capital city', examples=['Moscow', 'Santiago']),\n",
    "                no_branch=Node(summary='a city which is not a capital', examples=['Chicago', 'Tianjin'])\n",
    "            ),\n",
    "            no_branch=Node(\n",
    "                examples=['Oceania', 'Nevada', 'Times Square', 'Colosseum'],\n",
    "                question=\"Is it either a continent or a state of a country?\",\n",
    "                yes_branch=Node(\n",
    "                    examples=['North America', 'Nevada', 'Roraima', 'Uttar Pradesh'],\n",
    "                    question=\"Is it a continent?\",\n",
    "                    yes_branch=Node(summary='a continent', examples=['Africa', 'Asia']),\n",
    "                    no_branch=Node(summary='a state of a country', examples=['Kentucky', 'Santa Catarina Brazil'])\n",
    "                ),\n",
    "                no_branch=Node(\n",
    "                    examples=['Copacabana beach', 'university campus', 'Antarctica', 'tundra'],\n",
    "                    question=\"Is it a location made by humans?\",\n",
    "                    yes_branch=Node(summary='a location made by humans', examples=['Champs-Elysees', 'Amusement park']),\n",
    "                    no_branch=Node(summary='a natural location', examples=['Amazon rainforest', 'Desert'])\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    no_branch=Node(\n",
    "        examples=['Pencil sharpener', 'cloud', 'chimney', 'electric vehicle'],\n",
    "        question=\"Is it man-made?\",\n",
    "        tag=\"man-made\",\n",
    "        yes_branch=Node(\n",
    "            examples=['plastic bottle', 'martini', 'earbuds', 'helicopter'],\n",
    "            question=\"Is it found indoors?\",\n",
    "            tag=\"indoors\",\n",
    "            yes_branch=Node(\n",
    "                examples=['pencil sharpener', 'refrigerator', 'coffee machine', 'escalator'],\n",
    "                question=\"Is it something found in a home?\",\n",
    "                tag=\"home\",\n",
    "                yes_branch=Node(summary='a man-made thing found in people\\'s homes', examples=[\"Bed frame\", \"Stove\"]),\n",
    "                no_branch=Node(summary='a man-made thing found indoors but not in people\\'s homes', examples=[\"Hospital bed\", \"Erlenmeyer flask\"])\n",
    "            ),\n",
    "            no_branch=Node(\n",
    "                examples=['pickup truck', 'train', 'fountain', 'obelisk'],\n",
    "                question=\"Is it something found in a city?\",\n",
    "                tag=\"city\",\n",
    "                yes_branch=Node(summary='a man-made thing found outdoors in cities', examples=['Apartment Building', 'Fire Hydrant']),\n",
    "                no_branch=Node(summary='a man-made thing found outdoors outside of cities', examples=['Satellite', 'Barn'])\n",
    "            ),\n",
    "        ),\n",
    "        no_branch=Node(\n",
    "            examples=['thunder', 'Acacia', 'giraffe', 'river'],\n",
    "            question=\"Is it a living thing?\",\n",
    "            tag=\"living\",\n",
    "            yes_branch=Node(\n",
    "                examples=['Palm tree', 'beetle', 'whale', 'apple'],\n",
    "                question=\"Is it an animal?\",\n",
    "                tag=\"animal\",\n",
    "                yes_branch=Node(\n",
    "                    examples=['sea lion', 'cape toad', 'kangaroo', 'peacock'],\n",
    "                    question=\"Is it a mammal or a bird?\",\n",
    "                    yes_branch=Node(summary='either a mammal or a bird', examples=[\"Elephant\", \"Seagull\"]),\n",
    "                    no_branch=Node(summary='either a fish, insect, reptilian or amphibian', examples=[\"Shark\", \"Wasp\", \"Frog\", \"Alligator\"])\n",
    "                ),\n",
    "                no_branch=Node(\n",
    "                    examples=['mold', 'deciduous tree', 'yeast', 'agave'],\n",
    "                    question=\"Is it a plant?\",\n",
    "                    tag=\"plant\",\n",
    "                    yes_branch=Node(\n",
    "                        examples=['orchid', 'poison ivy', 'maple tree', 'sunflower'],\n",
    "                        question=\"Is it a tree?\",\n",
    "                        yes_branch=Node(summary='a tree', examples=[\"Oak tree\", \"Cactus\"]),\n",
    "                        no_branch=Node(summary='a plant which is not a tree', examples=[\"Rose\", \"Fern\"])\n",
    "                    ),\n",
    "                    no_branch=Node(\n",
    "                        examples=['black mold', 'truffle', 'cremini mushroom', 'bacillus'],\n",
    "                        question=\"Is it a fungus?\",\n",
    "                        yes_branch=Node(summary='a fungus', examples=[\"Wood ear mushroom\", \"Yeast\"]),\n",
    "                        no_branch=Node(summary='a bacteria', examples=['E Coli', 'Streptococcus'])\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            no_branch=Node(\n",
    "                examples=['uranus', 'seine river', 'carbon', 'electron'],\n",
    "                question=\"Is it a geological feature?\",\n",
    "                tag=\"geological feature\",\n",
    "                yes_branch=Node(\n",
    "                    examples=['Mount Fuji', 'volcano', 'canyon', 'Rocky mountains'],\n",
    "                    question=\"Is it a body of water?\",\n",
    "                    yes_branch=Node(summary='a body of water', examples=[\"River\", \"Mediterranean Sea\"]),\n",
    "                    no_branch=Node(summary='a geological feature which is not a body of water', examples=[\"Cave\", \"Volcano\"])\n",
    "                ),\n",
    "                no_branch=Node(\n",
    "                    examples=['mercury', 'sun', 'iron', 'lead'],\n",
    "                    question=\"Is it a chemical element, molecule or compound?\",\n",
    "                    yes_branch=Node(summary='a chemical element, molecule or compound', examples=[\"Salt\", \"Oxygen\"]),\n",
    "                    no_branch=Node(summary='a non-living thing not made by humans', examples=['Pebble', 'Milky Way'])\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the decision tree\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56c5d4",
   "metadata": {},
   "source": [
    "Open keywords data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3015193-ee9b-4c50-a193-ee1af0c6fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keywords in data: 2046\n"
     ]
    }
   ],
   "source": [
    "def read_file_to_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "keywords_list = read_file_to_list('20qs-data/keywords.txt')\n",
    "keywords_list = keywords_list[1:]\n",
    "print(f\"Number of keywords in data: {len(keywords_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4923da1",
   "metadata": {},
   "source": [
    "## Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d5c18b-e494-4349-8158-cbf86c9de815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_answer_from_node(node, keyword):\n",
    "    \n",
    "    prompt = f\"Keyword: {keyword}\\n\\nQuestion: {node.question}\\n\\n\"\n",
    "    prompt += \"\\nPlease provide an answer to the question based on the keyword. ONLY ANSWER Yes OR No. IF UNSURE, CHOOSE MOST LIKELY ANSWER FROM Yes OR No.\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def label_keywords(node, keywords, labels, results):\n",
    "    \n",
    "    question_tag = node.tag\n",
    "    yes_no_counts = Counter()\n",
    "\n",
    "    for keyword in tqdm(keywords, desc = 'label_keywords', position = 0, leave = True, ncols = 100):\n",
    "        answer = get_answer_from_node(node, keyword)\n",
    "        answer = answer.lower()\n",
    "        # Count yes/no answers\n",
    "        if 'yes' in answer:\n",
    "            yes_no_counts['yes'] += 1\n",
    "            answer = 'yes'\n",
    "        elif 'no' in answer:\n",
    "            yes_no_counts['no'] += 1\n",
    "            answer = 'no'\n",
    "        else:\n",
    "            yes_no_counts['err'] +=1\n",
    "            answer = 'err'\n",
    "        if keyword in labels:\n",
    "            labels[keyword][question_tag] = answer\n",
    "        else:\n",
    "            labels[keyword] = {question_tag: answer}\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_answers = sum(yes_no_counts.values())\n",
    "    yes_percentage = (yes_no_counts['yes'] / total_answers) * 100 if total_answers > 0 else 0\n",
    "    no_percentage = (yes_no_counts['no'] / total_answers) * 100 if total_answers > 0 else 0\n",
    "    err_percentage = (yes_no_counts['err'] / total_answers) * 100 if total_answers > 0 else 0\n",
    "\n",
    "    # Print results\n",
    "    print(node.question)\n",
    "    print(f\"Yes answers: {yes_no_counts['yes']}, percentage: {yes_percentage:.2f}%\")\n",
    "    print(f\"No answers: {yes_no_counts['no']}, percentage: {no_percentage:.2f}%\")\n",
    "    print(f\"Error answers: {yes_no_counts['err']}, percentage: {err_percentage:.2f}%\")\n",
    "\n",
    "    # Save results\n",
    "    results[question_tag] = {\n",
    "        'yes_percentage': yes_percentage,\n",
    "        'no_percentage': no_percentage,\n",
    "        'value_counts': dict(yes_no_counts)\n",
    "    }\n",
    "\n",
    "    return labels, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54f3bc-6332-44d3-a7e6-56928fdb809a",
   "metadata": {},
   "source": [
    "Read current keyword labels and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5588c21-afad-4457-b36e-c12a3d7e1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl_and_transform(file_path):\n",
    "    result_dict = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            keyword = record.pop('key')\n",
    "            result_dict[keyword] = record\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def read_json_to_dict(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "keyword_labels, results = read_jsonl_and_transform('data/labeled_keywords.jsonl'), read_json_to_dict('data/labeling_results.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0701435-953d-41fc-b567-72aa6c2e95ed",
   "metadata": {},
   "source": [
    "Label places/things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6bd5a7d-afcc-46ef-bc1a-68e81293308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it a place?\n",
      "Yes answers: 719, percentage: 35.15892420537897%\n",
      "No answers: 1326, percentage: 64.84107579462103%\n",
      "Error answers: 0, percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "keyword_labels, results = label_keywords(tree, keywords_list, keyword_labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6456aeae-fe98-46d2-85e8-c90b95804584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_places = [keyword for keyword in keywords_list if keyword_labels[keyword]['place'] == 'yes']\n",
    "keyword_things = [keyword for keyword in keywords_list if keyword_labels[keyword]['place'] == 'no']\n",
    "things_branch = tree.no_branch\n",
    "places_branch = tree.yes_branch\n",
    "\n",
    "print(f\"Number of keywords labeled as things: {len(keyword_things)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71441f18",
   "metadata": {},
   "source": [
    "Label man-made living:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5f2d6ab-d969-4a96-9bc9-d8ef06299ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it a living thing?\n",
      "Yes answers: 182, percentage: 64.31095406360424%\n",
      "No answers: 101, percentage: 35.68904593639576%\n",
      "Error answers: 0, percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "keyword_labels, results = label_keywords(not_man_made_branch, keyword_not_man_made, keyword_labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e5ac266e-e75d-495b-84e6-671f9e23bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_living = [keyword for keyword in keyword_not_man_made if keyword_labels[keyword]['living'] == 'yes']\n",
    "keyword_not_living = [keyword for keyword in keyword_not_man_made if keyword_labels[keyword]['living'] == 'no']\n",
    "living_branch = not_man_made_branch.yes_branch\n",
    "not_living_branch = not_man_made_branch.no_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962877df",
   "metadata": {},
   "source": [
    "Label indoors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83d5af0e-d850-45fe-9b43-bb2f01aea347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it found indoors?\n",
      "Yes answers: 912, percentage: 87.44%\n",
      "No answers: 131, percentage: 12.56%\n",
      "Error answers: 0, percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "keyword_labels, results = label_keywords(man_made_branch, keyword_man_made, keyword_labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "788c378e-0b42-458e-9c05-eac240007349",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_indoors = [keyword for keyword in keyword_man_made if keyword_labels[keyword]['indoors'] == 'yes']\n",
    "keyword_outdoors = [keyword for keyword in keyword_man_made if keyword_labels[keyword]['indoors'] == 'no']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f7332",
   "metadata": {},
   "source": [
    "Categorical labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37fabb5c-d2ee-463b-8ce4-cf27ae522477",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_nodes = [\n",
    "    Node(question='Is it broadly related to food, drinks or cooking?', tag='food, drinks, cooking'),\n",
    "    Node(question='Is it broadly related to arts, sports or entertainment?', tag='arts, sports, entertainment'),\n",
    "    Node(question='Is it broadly related to clothing, beauty or accessories?', tag='clothing, beauty, accessories'),\n",
    "    Node(question='Is it broadly related to furniture or architecture?', tag='furniture, architecture'),\n",
    "    Node(question='Is it broadly related to cleaning or hygiene?', tag='cleaning, hygiene'),\n",
    "    Node(question='Is it broadly related to transportation or vehicles?', tag='transportation, vehicles'),\n",
    "    Node(question='Is it broadly related to electronics or technology?', tag='electronics, technology'),    \n",
    "    Node(question='Is it broadly related to agriculture or industry?', tag='agriculture, industry'),\n",
    "    Node(question='Is it broadly related to science, school or education?', tag='science, school, education'),\n",
    "    Node(question='Is it broadly related to health or safety?', tag='health, safety'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d2c100d8-2968-4a59-8955-e8dfd73ffe5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords: 100%|███████████████████████████████████████████| 1326/1326 [12:19<00:00,  1.79it/s]                           | 0/10 [00:00<?, ?it/s]\n",
      "categories:  10%|█████████▉                                                                                         | 1/10 [12:19<1:50:51, 739.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it broadly related to food, drinks or cooking?\n",
      "Yes answers: 392, percentage: 29.56%\n",
      "No answers: 934, percentage: 70.44%\n",
      "Error answers: 0, percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords: 100%|███████████████████████████████████████████| 1326/1326 [11:57<00:00,  1.85it/s]\n",
      "categories:  20%|███████████████████▊                                                                               | 2/10 [24:16<1:36:52, 726.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it broadly related to arts, sports or entertainment?\n",
      "Yes answers: 439, percentage: 33.11%\n",
      "No answers: 887, percentage: 66.89%\n",
      "Error answers: 0, percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords: 100%|███████████████████████████████████████████| 1326/1326 [12:11<00:00,  1.81it/s]\n",
      "categories:  30%|█████████████████████████████▋                                                                     | 3/10 [36:28<1:25:01, 728.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it broadly related to clothing, beauty or accessories?\n",
      "Yes answers: 217, percentage: 16.37%\n",
      "No answers: 1109, percentage: 83.63%\n",
      "Error answers: 0, percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords: 100%|███████████████████████████████████████████| 1326/1326 [11:35<00:00,  1.91it/s]\n",
      "categories:  40%|███████████████████████████████████████▌                                                           | 4/10 [48:03<1:11:34, 715.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it broadly related to furniture or architecture?\n",
      "Yes answers: 172, percentage: 12.97%\n",
      "No answers: 1154, percentage: 87.03%\n",
      "Error answers: 0, percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords: 100%|█████████████████████████████████████████| 1326/1326 [1:30:17<00:00,  4.09s/it]\n",
      "categories:  50%|████████████████████████████████████████████████                                                | 5/10 [2:18:21<3:20:56, 2411.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it broadly related to cleaning or hygiene?\n",
      "Yes answers: 295, percentage: 22.25%\n",
      "No answers: 1031, percentage: 77.75%\n",
      "Error answers: 0, percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords:   0%|                                            | 1/1326 [00:15<5:32:51, 15.07s/it]\n",
      "categories:  50%|████████████████████████████████████████████████                                                | 5/10 [2:18:36<2:18:36, 1663.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1025\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat_node \u001b[38;5;129;01min\u001b[39;00m tqdm(category_nodes, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     keyword_labels, results \u001b[38;5;241m=\u001b[39m label_keywords(cat_node, keyword_things, keyword_labels, results)\n",
      "Cell \u001b[0;32mIn[148], line 28\u001b[0m, in \u001b[0;36mlabel_keywords\u001b[0;34m(node, keywords, labels, results)\u001b[0m\n\u001b[1;32m     25\u001b[0m yes_no_counts \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m tqdm(keywords, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_keywords\u001b[39m\u001b[38;5;124m'\u001b[39m, position \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, ncols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     answer \u001b[38;5;241m=\u001b[39m get_answer_from_node(node, keyword)\n\u001b[1;32m     29\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Count yes/no answers\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[148], line 10\u001b[0m, in \u001b[0;36mget_answer_from_node\u001b[0;34m(node, keyword)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyword: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide an answer to the question based on the keyword. ONLY ANSWER Yes OR No. IF UNSURE, CHOOSE MOST LIKELY ANSWER FROM Yes OR No.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     13\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     14\u001b[0m     ],\n\u001b[1;32m     15\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:646\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    645\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    648\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    649\u001b[0m             {\n\u001b[1;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    673\u001b[0m             },\n\u001b[1;32m    674\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    675\u001b[0m         ),\n\u001b[1;32m    676\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    677\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    678\u001b[0m         ),\n\u001b[1;32m    679\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    680\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    681\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    682\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    943\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    944\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    945\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    946\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    947\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    948\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1032\u001b[0m         input_options,\n\u001b[1;32m   1033\u001b[0m         cast_to,\n\u001b[1;32m   1034\u001b[0m         retries,\n\u001b[1;32m   1035\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1036\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1037\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1077\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1073\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1077\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1080\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1081\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1085\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cat_node in tqdm(category_nodes, desc = 'categories', ncols=150):\n",
    "    keyword_labels, results = label_keywords(cat_node, keyword_things, keyword_labels, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70501b05-af92-4f50-968f-41af8fcfe821",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Finish adding the labels below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27521a06-f7d1-4fa7-9aff-0234f1690f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords:  30%|█████████████▎                              | 402/1327 [09:04<20:52,  1.35s/it]                            | 0/4 [00:00<?, ?it/s]\n",
      "categories:   0%|                                                                                                               | 0/4 [09:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1025\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat_node \u001b[38;5;129;01min\u001b[39;00m tqdm(category_nodes[\u001b[38;5;241m6\u001b[39m:], desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     keyword_labels, results \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_things\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mlabel_keywords\u001b[0;34m(node, keywords, labels, results)\u001b[0m\n\u001b[1;32m     25\u001b[0m yes_no_counts \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m tqdm(keywords, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_keywords\u001b[39m\u001b[38;5;124m'\u001b[39m, position \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, leave \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, ncols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mget_answer_from_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Count yes/no answers\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mget_answer_from_node\u001b[0;34m(node, keyword)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyword: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide an answer to the question based on the keyword. ONLY ANSWER Yes OR No. IF UNSURE, CHOOSE MOST LIKELY ANSWER FROM Yes OR No.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:646\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    645\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1077\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1073\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1077\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1080\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1081\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1085\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cat_node in tqdm(category_nodes[6:], desc = 'categories', ncols=150):\n",
    "    keyword_labels, results = label_keywords(cat_node, keyword_things, keyword_labels, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5da3a",
   "metadata": {},
   "source": [
    "## Categorical Labels Analysis\n",
    "- Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99ab22",
   "metadata": {},
   "source": [
    "## Optimal Game Path with MCTS\n",
    "- Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48864e-6b8f-4340-a4f5-737565826b6b",
   "metadata": {},
   "source": [
    "## Save as JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b0ac208-503c-4530-9ba3-6f05bd792e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_from_dict(original_dict):\n",
    "    list_of_dicts = []\n",
    "    for key, sub_dict in original_dict.items():\n",
    "        # Create a new dictionary that includes the key from the original dict\n",
    "        new_dict = {\"key\": key}\n",
    "        # Update the new dictionary with the key-value pairs from the sub-dictionary\n",
    "        new_dict.update(sub_dict)\n",
    "        # Append the new dictionary to the list\n",
    "        list_of_dicts.append(new_dict)\n",
    "    return list_of_dicts\n",
    "\n",
    "labeled_keywords_list = make_list_from_dict(keyword_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "101a36c3-29e4-4079-98f5-41b97b5fddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"20qs-data/labeled_keywords.jsonl\", \"w\") as f:\n",
    "    for i in labeled_keywords_list:\n",
    "        json.dump(i, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(\"20qs-data/labeling_results.jsonl\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
