{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2d1b06-e8b0-434e-9de5-25b504712403",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!{sys.executable} -m pip install openai\n",
    "!{sys.executable} -m pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3715cd-c529-4091-b517-a0ba4d8c44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "if not os.environ[\"OPENAI_API_KEY\"]: \n",
    "    os.environ[\"OPENAI_API_KEY\"]= '<REDACTED>'\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56c5d4",
   "metadata": {},
   "source": [
    "Open keywords data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3015193-ee9b-4c50-a193-ee1af0c6fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keywords in data: 2046\n"
     ]
    }
   ],
   "source": [
    "def read_file_to_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "keywords_list = read_file_to_list('20qs-data/keywords.txt')\n",
    "keywords_list = keywords_list[1:]\n",
    "print(f\"Number of keywords in data: {len(keywords_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4923da1",
   "metadata": {},
   "source": [
    "## Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d5c18b-e494-4349-8158-cbf86c9de815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_answer(label, keyword):\n",
    "    \n",
    "    prompt = f\"Keyword: {keyword}\\n\\nQuestion: {label['question']}\\n\\n\"\n",
    "    prompt += \"Please provide an answer to the question based on the keyword. ONLY ANSWER Yes OR No. IF UNSURE, CHOOSE MOST LIKELY ANSWER FROM Yes OR No. \"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def label_keywords(label, keywords, labels, results):\n",
    "    \n",
    "    label_tag = label['tag']\n",
    "    yes_no_counts = Counter()\n",
    "\n",
    "    for keyword in tqdm(keywords, desc = 'label_keywords', position = 0, leave = True, ncols = 100):\n",
    "        answer = get_answer(label, keyword)\n",
    "        answer = answer.lower()\n",
    "        # Count yes/no answers\n",
    "        if 'yes' in answer:\n",
    "            yes_no_counts['yes'] += 1\n",
    "            answer = 'yes'\n",
    "        elif 'no' in answer:\n",
    "            yes_no_counts['no'] += 1\n",
    "            answer = 'no'\n",
    "        else:\n",
    "            yes_no_counts['err'] +=1\n",
    "            answer = 'err'\n",
    "        if keyword in labels:\n",
    "            labels[keyword][label_tag] = answer\n",
    "        else:\n",
    "            labels[keyword] = {label_tag: answer}\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_answers = sum(yes_no_counts.values())\n",
    "    yes_percentage = (yes_no_counts['yes'] / total_answers) * 100 if total_answers > 0 else 0\n",
    "    no_percentage = (yes_no_counts['no'] / total_answers) * 100 if total_answers > 0 else 0\n",
    "    err_percentage = (yes_no_counts['err'] / total_answers) * 100 if total_answers > 0 else 0\n",
    "\n",
    "    # Print results\n",
    "    print(label['question'])\n",
    "    print(f\"Yes answers: {yes_no_counts['yes']}, percentage: {yes_percentage:.2f}%\")\n",
    "    print(f\"No answers: {yes_no_counts['no']}, percentage: {no_percentage:.2f}%\")\n",
    "    print(f\"Error answers: {yes_no_counts['err']}, percentage: {err_percentage:.2f}%\")\n",
    "\n",
    "    # Save results\n",
    "    results[label_tag] = {\n",
    "        'yes_percentage': yes_percentage,\n",
    "        'no_percentage': no_percentage,\n",
    "        'yes_counts': yes_no_counts['yes'],\n",
    "        'no_counts': yes_no_counts['no'],\n",
    "        'err_counts': yes_no_counts['err']\n",
    "    }\n",
    "\n",
    "    return labels, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54f3bc-6332-44d3-a7e6-56928fdb809a",
   "metadata": {},
   "source": [
    "Read current keyword labels and results for adding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5588c21-afad-4457-b36e-c12a3d7e1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl_and_transform(file_path):\n",
    "    result_dict = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            keyword = record.pop('keyword')\n",
    "            result_dict[keyword] = record\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def read_json_to_dict(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "keyword_labels, results = read_jsonl_and_transform('20qs-data/labeled_keywords.jsonl'), read_json_to_dict('20qs-data/labeling_results.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c6446",
   "metadata": {},
   "source": [
    "OR start without any labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fad6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_labels = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0701435-953d-41fc-b567-72aa6c2e95ed",
   "metadata": {},
   "source": [
    "Label places/things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6bd5a7d-afcc-46ef-bc1a-68e81293308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords: 100%|███████████████████████████████████████████| 2046/2046 [16:02<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it a place?\n",
      "Yes answers: 710, percentage: 34.70%\n",
      "No answers: 1336, percentage: 65.30%\n",
      "Error answers: 0, percentage: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "places_label = {'tag': 'place', 'question': \"Is it a place?\"}\n",
    "\n",
    "keyword_labels, results = label_keywords(places_label, keywords_list, keyword_labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6456aeae-fe98-46d2-85e8-c90b95804584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keywords labeled as things: 1336.\n"
     ]
    }
   ],
   "source": [
    "keyword_places = [keyword for keyword in keywords_list if keyword_labels[keyword]['place'] == 'yes']\n",
    "keyword_things = [keyword for keyword in keywords_list if keyword_labels[keyword]['place'] == 'no']\n",
    "\n",
    "print(f\"Number of keywords labeled as things: {len(keyword_things)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3f91c",
   "metadata": {},
   "source": [
    "## Things Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f7332",
   "metadata": {},
   "source": [
    "Labels to apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fabb5c-d2ee-463b-8ce4-cf27ae522477",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    {'tag': 'scientific research', 'question': \"Is it related to scientific research?\"},\n",
    "    {'tag': 'food, drinks, cooking', 'question': \"Is it broadly related to food, drinks or cooking?\"},\n",
    "    {'tag': 'handheld', 'question': \"Is it something that can be held in someone's hand?\"},\n",
    "    {'tag': 'food', 'question': \"Is it a food?\"},\n",
    "    {'tag': 'furniture', 'question': \"Is it furniture?\"},\n",
    "    {'tag': 'man-made', 'question': \"Is it man-made?\"},\n",
    "    {'tag': 'industry, manufacturing', 'question': \"Is it related to industrial production or manufacturing?\"},\n",
    "    {'tag': 'agriculture', 'question': \"Is it related to agricultural production?\"},\n",
    "    {'tag': 'arts, media', 'question': \"Is it broadly related to arts or media?\"},\n",
    "    {'tag': 'safety', 'question': \"Is it related to safety?\"},\n",
    "    {'tag': 'medicine', 'question': \"Is it broadly related to medicine?\"},\n",
    "    {'tag': 'transportation, vehicles', 'question': \"Is it related to transportation or vehicles?\"},\n",
    "    {'tag': 'electronics, technology', 'question': \"Is it related to electronics or technology?\"},\n",
    "    {'tag': 'clothing, accessories', 'question': \"Is it related to clothing or accessories?\"},\n",
    "    {'tag': 'natural resource, material', 'question': \"Is it a natural resource or material?\"},\n",
    "    {'tag': 'natural phenomenon', 'question': \"Is it a natural phenomenon or natural feature?\"},\n",
    "    {'tag': 'living', 'question': \"Is it a living thing?\"},\n",
    "    {'tag': 'animal', 'question': \"Is it an animal?\"},\n",
    "    {'tag': 'plant', 'question': \"Is it a plant?\"},\n",
    "    {'tag': 'cleaning, hygiene', 'question': \"Is it related to cleaning or hygiene?\"},\n",
    "    {'tag': 'entertainment, sports', 'question': \"Is it broadly related to entertainment or sports?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c100d8-2968-4a59-8955-e8dfd73ffe5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "labels:   0%|                                                                                                                  | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label_keywords:   0%|                                                      | 0/1336 [00:03<?, ?it/s]\n",
      "labels:   0%|                                                                                                                  | 0/21 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1025\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m tqdm(labels, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     keyword_labels, results \u001b[38;5;241m=\u001b[39m label_keywords(label, keyword_things, keyword_labels, results)\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mlabel_keywords\u001b[0;34m(label, keywords, labels, results)\u001b[0m\n\u001b[1;32m     24\u001b[0m yes_no_counts \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m tqdm(keywords, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_keywords\u001b[39m\u001b[38;5;124m'\u001b[39m, position \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, leave \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, ncols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     answer \u001b[38;5;241m=\u001b[39m get_answer(label, keyword)\n\u001b[1;32m     28\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Count yes/no answers\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mget_answer\u001b[0;34m(label, keyword)\u001b[0m\n\u001b[1;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyword: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide an answer to the question based on the keyword. ONLY ANSWER Yes OR No. IF UNSURE, CHOOSE MOST LIKELY ANSWER FROM Yes OR No. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     12\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     13\u001b[0m     ],\n\u001b[1;32m     14\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:646\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    645\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    648\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    649\u001b[0m             {\n\u001b[1;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    673\u001b[0m             },\n\u001b[1;32m    674\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    675\u001b[0m         ),\n\u001b[1;32m    676\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    677\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    678\u001b[0m         ),\n\u001b[1;32m    679\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    680\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    681\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    682\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    943\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    944\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    945\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    946\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    947\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    948\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1030\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1032\u001b[0m         input_options,\n\u001b[1;32m   1033\u001b[0m         cast_to,\n\u001b[1;32m   1034\u001b[0m         retries,\n\u001b[1;32m   1035\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1036\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1037\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1077\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1073\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1077\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1080\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1081\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1085\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for label in tqdm(labels, desc = 'labels', ncols=150):\n",
    "    keyword_labels, results = label_keywords(label, keyword_things, keyword_labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a1da280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accent chair', 'adjustable bench', 'Advertisement', 'air handler', 'Alarm system', 'amazon echo', 'analog stick', 'ankle bracelet', 'aperol spritz', 'Apple pie', 'Aprons', 'archery target', 'arrival board', 'atm', 'attic ladder', 'audio guide', 'autograph', 'backgammon board', 'baggage conveyor', 'Baguette', 'ballpoint pen', 'bandana', 'bank statement', 'bar towel', 'barbell collar', 'Barber Chair', 'Bath Mat', 'beach chair', 'beach umbrella', 'Beanbag', 'Bed frame', 'bedspread', 'beer coaster', 'beer stein', 'beer tap', 'Blinds', 'Bobby Pins', 'Bollards', 'book cover', 'bookbag', 'bookend', 'Bookends', 'bracelet', 'bread basket', 'Bread knife', 'bread maker', 'Bread pudding', 'Brewery merchandise', 'Briefcase', 'brochure stand', 'Brownies', 'buckle', 'Bumper sticker', 'bunk bed', 'butter knife', 'button', 'cabinet', 'cable tie', 'cable tray', 'camp stove', 'candle holder', 'canning jar', 'cannoli', 'Cash Register', 'casket', 'Cat carrier', 'centerpiece', 'chair cushion', 'Champagne flute', 'Chandelier', 'charging cord', 'Cheesecake', 'chicken wire', 'chime', 'chutney', 'cinder block', 'Cinnamon roll', 'climbing rope', 'clutch bag', 'Coat Rack', 'cobblestone', 'cocktail glass', 'Coffee Grinder', 'Coffee Makers', 'coffee mug', 'coffee table', 'coffin', 'colander', 'Comic Book', 'compactor', 'cordless phone', 'corkscrew', 'cowboy boot', 'crochet hook', 'Croissant', 'cruise ship', 'crutch', 'cuff', 'cupcake', 'curler']\n"
     ]
    }
   ],
   "source": [
    "keyword_science = [keyword for keyword in keyword_things if keyword_labels[keyword]['scientific research'] == 'yes']\n",
    "keyword_not_science = [keyword for keyword in keyword_things if keyword_labels[keyword]['scientific research'] == 'no']\n",
    "print(keyword_not_science[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134d2c5",
   "metadata": {},
   "source": [
    "## Places Branch\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48864e-6b8f-4340-a4f5-737565826b6b",
   "metadata": {},
   "source": [
    "## Save as JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0ac208-503c-4530-9ba3-6f05bd792e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_from_dict(original_dict):\n",
    "    list_of_dicts = []\n",
    "    for key, sub_dict in original_dict.items():\n",
    "        # Create a new dictionary that includes the key from the original dict\n",
    "        new_dict = {\"keyword\": key}\n",
    "        # Update the new dictionary with the key-value pairs from the sub-dictionary\n",
    "        new_dict.update(sub_dict)\n",
    "        # Append the new dictionary to the list\n",
    "        list_of_dicts.append(new_dict)\n",
    "    return list_of_dicts\n",
    "\n",
    "labeled_keywords_list = make_list_from_dict(keyword_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "101a36c3-29e4-4079-98f5-41b97b5fddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"20qs-data/labeled_keywords.jsonl\", \"w\") as f:\n",
    "    for i in labeled_keywords_list:\n",
    "        json.dump(i, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(\"20qs-data/labeling_results.jsonl\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
